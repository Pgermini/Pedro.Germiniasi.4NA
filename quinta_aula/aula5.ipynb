{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['   Bonjour, comment ca va ?     ',\n",
       " '    Heyyyyy, how are you doing ?   ',\n",
       " '        Hallo, wie gehts ?     ']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    '   Bonjour, comment ca va ?     ',\n",
    "    '    Heyyyyy, how are you doing ?   ',\n",
    "    '        Hallo, wie gehts ?     '\n",
    "]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour, comment ca va ?',\n",
       " 'Heyyyyy, how are you doing ?',\n",
       " 'Hallo, wie gehts ?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[text.strip() for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"abcd Who is abcd ? That's not a real name!!! abcd\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"abcd Who is abcd ? That's not a real name!!! abcd\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Who is abcd ? That's not a real name!!! \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.strip('bdac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love koalas, koalas are the cutest animals on Earth.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'linkin park / metallica /red hot chili peppers'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.replace(\"koala\", \"panda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"linkin park / metallica /red hot chili peppers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"linkin park / metallica /red hot chili peppers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linkin park ', ' metallica ', 'red hot chili peppers']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i LOVE football sO mUch. FOOTBALL is my passion. Who else loves fOOtBaLL ?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"i LOVE football sO mUch. FOOTBALL is my passion. Who else loves fOOtBaLL ?\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love football so much. football is my passion. who else loves football ?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not recommend this restaurant, we waited for so long, like 30 minutes, this is ridiculous'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"i do not recommend this restaurant, we waited for so long, like 30 minutes, this is ridiculous\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not recommend this restaurant, we waited for so long, like  minutes, this is ridiculous'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = ''.join(char for char in text if not char.isdigit())\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love bubble tea! OMG so #tasty @channel XOXO @$ ^_^ '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I love bubble tea! OMG so #tasty @channel XOXO @$ ^_^ \"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string # \"string\" module is already installed with Python\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love bubble tea OMG so tasty channel XOXO   '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for punctuation in string.punctuation:\n",
    "    text = text.replace(punctuation, '')\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"   I LOVE Pizza 999 @^_^\",\n",
    "    \"  Le Wagon is amazing, take care - 666\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '')\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love pizza', 'le wagon is amazing take care']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned = [basic_cleaning(sentence) for sentence in sentences]\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['darkvador@gmail.com', 'batman@outlook.com']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt = '''\n",
    "    This is a random text, authored by darkvador@gmail.com\n",
    "    and batman@outlook.com, WOW!\n",
    "'''\n",
    "\n",
    "re.findall('[\\w.+-]+@[\\w-]+\\.[\\w.-]+', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is during our darkest moments that we must focus to see the light'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'It is during our darkest moments that we must focus to see the light'\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'during', 'our', 'darkest', 'moments', 'that', 'we', 'must', 'focus', 'to', 'see', 'the', 'light']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Make sure the punkt package is properly downloaded and configured\n",
    "nltk.download('punkt', quiet=False)\n",
    "\n",
    "# Set NLTK to use a specific language model - try PY3 as it might be the Python 3 compatible version\n",
    "nltk.data.path.append('/home/codespace/nltk_data')\n",
    "\n",
    "# Explicitly try to use a specific model\n",
    "text = 'It is during our darkest moments that we must focus to see the light'\n",
    "\n",
    "# Alternative approach that doesn't rely on the punkt model\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "word_tokens = tokenizer.tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estivermos', 'tivéramos', 'pela', 'fui', 'hajamos', 'hei', 'fosse', 'sejamos', 'esta', 'tinham', 'te', 'aquele', 'tem', 'vos', 'aqueles', 'era', 'esteja', 'fomos', 'também', 'mais', 'minhas', 'hajam', 'seu', 'há', 'que', 'estavam', 'em', 'estou', 'estiveram', 'houvemos', 'elas', 'tivermos', 'estiverem', 'nós', 'tivessem', 'houvéssemos', 'temos', 'ao', 'qual', 'esteve', 'haja', 'seremos', 'seríamos', 'deles', 'houverei', 'as', 'muito', 'houveram', 'tive', 'tenha', 'estivéssemos', 'estejam', 'serão', 'houveria', 'teríamos', 'estão', 'teus', 'tivéssemos', 'tenham', 'como', 'estiver', 'tém', 'houvermos', 'minha', 'houvera', 'houvessem', 'é', 'estivemos', 'mesmo', 'foram', 'está', 'essa', 'seja', 'por', 'da', 'seriam', 'não', 'aquela', 'formos', 'nossas', 'essas', 'aquilo', 'ela', 'houvéramos', 'tiver', 'um', 'forem', 'das', 'vocês', 'esses', 'estes', 'lhe', 'houverão', 'para', 'até', 'tenho', 'eles', 'tiverem', 'quem', 'houverem', 'do', 'houvesse', 'estas', 'dos', 'entre', 'teria', 'houveriam', 'isso', 'somos', 'de', 'serei', 'lhes', 'pelos', 'tiveram', 'estava', 'a', 'estávamos', 'seus', 'tivera', 'às', 'tuas', 'já', 'houve', 'nossa', 'ser', 'este', 'eu', 'terá', 'teriam', 'tivemos', 'só', 'teve', 'houveríamos', 'havemos', 'for', 'hão', 'nem', 'suas', 'os', 'éramos', 'depois', 'fôramos', 'sua', 'nas', 'pelo', 'houver', 'numa', 'à', 'delas', 'estivera', 'esse', 'mas', 'tínhamos', 'haver', 'me', 'tu', 'fossem', 'ou', 'dela', 'estejamos', 'fora', 'isto', 'e', 'estivéramos', 'estamos', 'pelas', 'sejam', 'se', 'estivessem', 'tenhamos', 'num', 'tinha', 'estive', 'houverá', 'o', 'ele', 'são', 'tivesse', 'no', 'houveremos', 'teu', 'eram', 'nossos', 'fôssemos', 'teremos', 'seria', 'foi', 'será', 'terão', 'estar', 'aos', 'aquelas', 'meu', 'na', 'terei', 'meus', 'sou', 'nosso', 'quando', 'você', 'tua', 'dele', 'uma', 'sem', 'nos', 'com', 'estivesse'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'estamos',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéramos',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'está',\n",
       " 'estávamos',\n",
       " 'estão',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fui',\n",
       " 'fôramos',\n",
       " 'fôssemos',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'havemos',\n",
       " 'haver',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houveram',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houvermos',\n",
       " 'houverá',\n",
       " 'houverão',\n",
       " 'houveríamos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéramos',\n",
       " 'houvéssemos',\n",
       " 'há',\n",
       " 'hão',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'não',\n",
       " 'nós',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'ser',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'são',\n",
       " 'só',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéramos',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'tém',\n",
       " 'tínhamos',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos',\n",
       " 'à',\n",
       " 'às',\n",
       " 'é',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Baixar o recurso de stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Agora tente carregar as stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Exibir algumas stopwords\n",
    "print(stop_words)\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ellas', 'fui', 'nuestras', 'te', 'tenida', 'era', 'sentid', 'otro', 'tuvierais', 'fuiste', 'un', 'qué', 'estén', 'tened', 'ya', 'seríais', 'hubisteis', 'ha', 'seremos', 'hubieron', 'le', 'habías', 'estuviste', 'cuando', 'donde', 'estando', 'tendrían', 'los', 'sí', 'tanto', 'sus', 'estad', 'ni', 'tengan', 'habíais', 'habría', 'teniendo', 'tuyos', 'estuvierais', 'para', 'hubierais', 'vuestra', 'les', 'de', 'habríamos', 'sería', 'habríais', 'fueran', 'yo', 'tendréis', 'fuese', 'nuestra', 'estuvisteis', 'vuestro', 'tendrás', 'están', 'estuvieras', 'tuvieron', 'contra', 'mis', 'fuerais', 'esa', 'otros', 'tu', 'habrá', 'fueras', 'seas', 'ellos', 'lo', 'fuésemos', 'hube', 'o', 'cual', 'tuviesen', 'mí', 'tuviéramos', 'habremos', 'estar', 'tengo', 'tenéis', 'hubieseis', 'habiendo', 'estés', 'nos', 'teníais', 'tendría', 'tuvo', 'estéis', 'esta', 'estuviera', 'muy', 'serían', 'hay', 'que', 'serán', 'estos', 'tienen', 'sois', 'estarás', 'son', 'habréis', 'mía', 'otra', 'estuvo', 'ante', 'tuvieses', 'fuesen', 'tienes', 'tengas', 'hayan', 'fueron', 'seamos', 'hubimos', 'ti', 'había', 'hubieses', 'tengamos', 'hubiesen', 'quien', 'hayáis', 'siente', 'él', 'estado', 'tiene', 'fuéramos', 'tendremos', 'tendré', 'fuera', 'mi', 'tuviste', 'estoy', 'hubiéramos', 'sea', 'hubiera', 'tengáis', 'esas', 'fuimos', 'sentidas', 'nosotras', 'estadas', 'tuve', 'también', 'tuvieseis', 'suyo', 'habré', 'tuya', 'tenidas', 'estuvieron', 'será', 'otras', 'estuvieses', 'desde', 'esté', 'más', 'habrás', 'en', 'habrían', 'sentidos', 'tenías', 'la', 'habían', 'suyos', 'sean', 'teníamos', 'y', 'antes', 'mucho', 'habéis', 'sin', 'hubieran', 'seáis', 'tenía', 'tuvimos', 'tuvieran', 'eras', 'sintiendo', 'mío', 'tendrías', 'hubiésemos', 'han', 'habido', 'seríamos', 'estada', 'algunos', 'soy', 'fueseis', 'estuvieseis', 'con', 'algunas', 'serás', 'fueses', 'pero', 'está', 'por', 'habidas', 'tuviese', 'seré', 'algo', 'vuestras', 'estas', 'tenidos', 'entre', 'tendrá', 'estás', 'a', 'has', 'una', 'fuisteis', 'seréis', 'estarías', 'poco', 'estarán', 'estaré', 'estaban', 'estabais', 'tendríamos', 'todo', 'todos', 'he', 'estados', 'estaba', 'e', 'tendrán', 'esos', 'esto', 'hemos', 'el', 'ese', 'unos', 'estarían', 'hubiste', 'serías', 'estuviésemos', 'míos', 'hayamos', 'estuve', 'su', 'del', 'hasta', 'sobre', 'sentida', 'erais', 'es', 'durante', 'eran', 'estaríamos', 'haya', 'estaremos', 'suya', 'tenemos', 'eres', 'tuviésemos', 'estaríais', 'nada', 'tuyas', 'hubiese', 'quienes', 'fue', 'estemos', 'estuviesen', 'muchos', 'habrán', 'tus', 'uno', 'tuvieras', 'hubieras', 'somos', 'mías', 'tuviera', 'nosotros', 'nuestro', 'porque', 'hubo', 'este', 'habida', 'habíamos', 'estuviese', 'estaréis', 'estábamos', 'estuvieran', 'suyas', 'tenido', 'habidos', 'os', 'tuvisteis', 'éramos', 'las', 'estaría', 'eso', 'me', 'estáis', 'vuestros', 'al', 'hayas', 'estamos', 'nuestros', 'habrías', 'estuviéramos', 'se', 'tuyo', 'estará', 'ella', 'tenían', 'no', 'estuvimos', 'tú', 'sentido', 'vosotros', 'vosotras', 'estabas', 'tenga', 'tendríais', 'como'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'al',\n",
       " 'algo',\n",
       " 'algunas',\n",
       " 'algunos',\n",
       " 'ante',\n",
       " 'antes',\n",
       " 'como',\n",
       " 'con',\n",
       " 'contra',\n",
       " 'cual',\n",
       " 'cuando',\n",
       " 'de',\n",
       " 'del',\n",
       " 'desde',\n",
       " 'donde',\n",
       " 'durante',\n",
       " 'e',\n",
       " 'el',\n",
       " 'ella',\n",
       " 'ellas',\n",
       " 'ellos',\n",
       " 'en',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'eras',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'esa',\n",
       " 'esas',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'esos',\n",
       " 'esta',\n",
       " 'estaba',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estabas',\n",
       " 'estad',\n",
       " 'estada',\n",
       " 'estadas',\n",
       " 'estado',\n",
       " 'estados',\n",
       " 'estamos',\n",
       " 'estando',\n",
       " 'estar',\n",
       " 'estaremos',\n",
       " 'estará',\n",
       " 'estarán',\n",
       " 'estarás',\n",
       " 'estaré',\n",
       " 'estaréis',\n",
       " 'estaría',\n",
       " 'estaríais',\n",
       " 'estaríamos',\n",
       " 'estarían',\n",
       " 'estarías',\n",
       " 'estas',\n",
       " 'este',\n",
       " 'estemos',\n",
       " 'esto',\n",
       " 'estos',\n",
       " 'estoy',\n",
       " 'estuve',\n",
       " 'estuviera',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuvieras',\n",
       " 'estuvieron',\n",
       " 'estuviese',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estuvieses',\n",
       " 'estuvimos',\n",
       " 'estuviste',\n",
       " 'estuvisteis',\n",
       " 'estuviéramos',\n",
       " 'estuviésemos',\n",
       " 'estuvo',\n",
       " 'está',\n",
       " 'estábamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'estás',\n",
       " 'esté',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estés',\n",
       " 'fue',\n",
       " 'fuera',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fueras',\n",
       " 'fueron',\n",
       " 'fuese',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'fueses',\n",
       " 'fui',\n",
       " 'fuimos',\n",
       " 'fuiste',\n",
       " 'fuisteis',\n",
       " 'fuéramos',\n",
       " 'fuésemos',\n",
       " 'ha',\n",
       " 'habida',\n",
       " 'habidas',\n",
       " 'habido',\n",
       " 'habidos',\n",
       " 'habiendo',\n",
       " 'habremos',\n",
       " 'habrá',\n",
       " 'habrán',\n",
       " 'habrás',\n",
       " 'habré',\n",
       " 'habréis',\n",
       " 'habría',\n",
       " 'habríais',\n",
       " 'habríamos',\n",
       " 'habrían',\n",
       " 'habrías',\n",
       " 'habéis',\n",
       " 'había',\n",
       " 'habíais',\n",
       " 'habíamos',\n",
       " 'habían',\n",
       " 'habías',\n",
       " 'han',\n",
       " 'has',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'haya',\n",
       " 'hayamos',\n",
       " 'hayan',\n",
       " 'hayas',\n",
       " 'hayáis',\n",
       " 'he',\n",
       " 'hemos',\n",
       " 'hube',\n",
       " 'hubiera',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubieras',\n",
       " 'hubieron',\n",
       " 'hubiese',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'hubieses',\n",
       " 'hubimos',\n",
       " 'hubiste',\n",
       " 'hubisteis',\n",
       " 'hubiéramos',\n",
       " 'hubiésemos',\n",
       " 'hubo',\n",
       " 'la',\n",
       " 'las',\n",
       " 'le',\n",
       " 'les',\n",
       " 'lo',\n",
       " 'los',\n",
       " 'me',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'mucho',\n",
       " 'muchos',\n",
       " 'muy',\n",
       " 'más',\n",
       " 'mí',\n",
       " 'mía',\n",
       " 'mías',\n",
       " 'mío',\n",
       " 'míos',\n",
       " 'nada',\n",
       " 'ni',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nosotras',\n",
       " 'nosotros',\n",
       " 'nuestra',\n",
       " 'nuestras',\n",
       " 'nuestro',\n",
       " 'nuestros',\n",
       " 'o',\n",
       " 'os',\n",
       " 'otra',\n",
       " 'otras',\n",
       " 'otro',\n",
       " 'otros',\n",
       " 'para',\n",
       " 'pero',\n",
       " 'poco',\n",
       " 'por',\n",
       " 'porque',\n",
       " 'que',\n",
       " 'quien',\n",
       " 'quienes',\n",
       " 'qué',\n",
       " 'se',\n",
       " 'sea',\n",
       " 'seamos',\n",
       " 'sean',\n",
       " 'seas',\n",
       " 'sentid',\n",
       " 'sentida',\n",
       " 'sentidas',\n",
       " 'sentido',\n",
       " 'sentidos',\n",
       " 'seremos',\n",
       " 'será',\n",
       " 'serán',\n",
       " 'serás',\n",
       " 'seré',\n",
       " 'seréis',\n",
       " 'sería',\n",
       " 'seríais',\n",
       " 'seríamos',\n",
       " 'serían',\n",
       " 'serías',\n",
       " 'seáis',\n",
       " 'siente',\n",
       " 'sin',\n",
       " 'sintiendo',\n",
       " 'sobre',\n",
       " 'sois',\n",
       " 'somos',\n",
       " 'son',\n",
       " 'soy',\n",
       " 'su',\n",
       " 'sus',\n",
       " 'suya',\n",
       " 'suyas',\n",
       " 'suyo',\n",
       " 'suyos',\n",
       " 'sí',\n",
       " 'también',\n",
       " 'tanto',\n",
       " 'te',\n",
       " 'tendremos',\n",
       " 'tendrá',\n",
       " 'tendrán',\n",
       " 'tendrás',\n",
       " 'tendré',\n",
       " 'tendréis',\n",
       " 'tendría',\n",
       " 'tendríais',\n",
       " 'tendríamos',\n",
       " 'tendrían',\n",
       " 'tendrías',\n",
       " 'tened',\n",
       " 'tenemos',\n",
       " 'tenga',\n",
       " 'tengamos',\n",
       " 'tengan',\n",
       " 'tengas',\n",
       " 'tengo',\n",
       " 'tengáis',\n",
       " 'tenida',\n",
       " 'tenidas',\n",
       " 'tenido',\n",
       " 'tenidos',\n",
       " 'teniendo',\n",
       " 'tenéis',\n",
       " 'tenía',\n",
       " 'teníais',\n",
       " 'teníamos',\n",
       " 'tenían',\n",
       " 'tenías',\n",
       " 'ti',\n",
       " 'tiene',\n",
       " 'tienen',\n",
       " 'tienes',\n",
       " 'todo',\n",
       " 'todos',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'tuve',\n",
       " 'tuviera',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuvieras',\n",
       " 'tuvieron',\n",
       " 'tuviese',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'tuvieses',\n",
       " 'tuvimos',\n",
       " 'tuviste',\n",
       " 'tuvisteis',\n",
       " 'tuviéramos',\n",
       " 'tuviésemos',\n",
       " 'tuvo',\n",
       " 'tuya',\n",
       " 'tuyas',\n",
       " 'tuyo',\n",
       " 'tuyos',\n",
       " 'tú',\n",
       " 'un',\n",
       " 'una',\n",
       " 'uno',\n",
       " 'unos',\n",
       " 'vosotras',\n",
       " 'vosotros',\n",
       " 'vuestra',\n",
       " 'vuestras',\n",
       " 'vuestro',\n",
       " 'vuestros',\n",
       " 'y',\n",
       " 'ya',\n",
       " 'yo',\n",
       " 'él',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "print(stop_words)\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após limpeza básica: he was running and eating at the same time he has a bad habit of swimming after playing hours in the sun\n",
      "Após tokenização: ['he', 'was', 'running', 'and', 'eating', 'at', 'the', 'same', 'time', 'he', 'has', 'a', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'hours', 'in', 'the', 'sun']\n",
      "Após remoção de stopwords: ['running', 'eating', 'time', 'bad', 'habit', 'swimming', 'playing', 'hours', 'sun']\n",
      "Após lematização: ['running', 'eating', 'time', 'bad', 'habit', 'swimming', 'playing', 'hour', 'sun']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Baixar recursos necessários do NLTK\n",
    "nltk.download('stopwords', download_dir='/home/codespace/nltk_data')\n",
    "nltk.download('wordnet', download_dir='/home/codespace/nltk_data')\n",
    "nltk.download('omw-1.4', download_dir='/home/codespace/nltk_data')  # Open Multilingual WordNet\n",
    "nltk.data.path.append('/home/codespace/nltk_data')\n",
    "\n",
    "def basic_cleaning(text):\n",
    "    # Converter para minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remover pontuações\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remover números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remover espaços extras\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokeniza um texto dividindo por espaços\n",
    "    \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Remove stopwords da lista de tokens\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Lematiza uma lista de tokens usando WordNetLemmatizer do NLTK\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Exemplo de uso\n",
    "sentence = 'He was RUNNING and EATING at the same time =[. He has a bad habit of swimming after playing 3 hours in the Sun =/'\n",
    "\n",
    "# Etapa 1: Limpeza básica\n",
    "cleaned_sentence = basic_cleaning(sentence)\n",
    "print(\"Após limpeza básica:\", cleaned_sentence)\n",
    "\n",
    "# Etapa 2: Tokenização simples\n",
    "tokens = simple_tokenize(cleaned_sentence)\n",
    "print(\"Após tokenização:\", tokens)\n",
    "\n",
    "# Etapa 3: Remoção de stopwords\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(\"Após remoção de stopwords:\", filtered_tokens)\n",
    "\n",
    "# Etapa 4: Lematização\n",
    "lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "print(\"Após lematização:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após limpeza básica: he was running and eating at the same time he has a bad habit of swimming after playing hours in the sun\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'He was RUNNING and EATING at the same time =[. He has a bad habit of swimming after playing 3 hours in the Sun =/'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Etapa 1: Limpeza básica\n",
    "cleaned_sentence = basic_cleaning(sentence)\n",
    "print(\"Após limpeza básica:\", cleaned_sentence)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após tokenização: ['he', 'was', 'running', 'and', 'eating', 'at', 'the', 'same', 'time', 'he', 'has', 'a', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'hours', 'in', 'the', 'sun']\n"
     ]
    }
   ],
   "source": [
    "# Etapa 2: Tokenização simples\n",
    "tokens = simple_tokenize(cleaned_sentence)\n",
    "print(\"Após tokenização:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após remoção de stopwords: ['running', 'eating', 'time', 'bad', 'habit', 'swimming', 'playing', 'hours', 'sun']\n"
     ]
    }
   ],
   "source": [
    "# Etapa 3: Remoção de stopwords\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(\"Após remoção de stopwords:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após lematização: ['running', 'eating', 'time', 'bad', 'habit', 'swimming', 'playing', 'hour', 'sun']\n"
     ]
    }
   ],
   "source": [
    "# Etapa 4: Lematização\n",
    "lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "print(\"Após lematização:\", lemmatized_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
